#!/usr/bin/env python3
"""
LM Studio Auto-Evict å’Œ JIT Loading æ¸¬è©¦è…³æœ¬

æ­¤è…³æœ¬ç”¨æ–¼é©—è­‰ LM Studio çš„ Auto-Evict å’Œ JIT Loading é…ç½®æ˜¯å¦æ­£ç¢ºå•Ÿç”¨ã€‚
"""

import sys
import time
import requests
from typing import Dict, Any

# ç¢ºä¿èƒ½å¤  import Django è¨­å®š
import os
import django
sys.path.append('/app')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings.local')
django.setup()

from mcp.config import get_config


def test_auto_evict_configuration() -> None:
    """æ¸¬è©¦ Auto-Evict å’Œ JIT Loading é…ç½®"""
    
    config = get_config()
    chat_url = f"http://{config.lmstudio_host}:{config.lmstudio_port}/v1/chat/completions"
    
    print("=" * 60)
    print("ğŸ§ª LM Studio Auto-Evict å’Œ JIT Loading æ¸¬è©¦")
    print("=" * 60)
    
    # æ¸¬è©¦æ¨¡å‹åˆ—è¡¨
    test_models = [
        "deepseek-r1-distill-qwen-7b",
        "deepseek-r1-0528-qwen3-8b"
    ]
    
    # æ¸¬è©¦1: æª¢æŸ¥ç•¶å‰è¨˜æ†¶é«”ç‹€æ…‹
    print("\nğŸ“Š æ¸¬è©¦1: æª¢æŸ¥ç•¶å‰è¨˜æ†¶é«”ç‹€æ…‹")
    memory_status = check_memory_status(chat_url)
    print(f"çµæœ: {memory_status}")
    
    # æ¸¬è©¦2: JIT Loading æ¸¬è©¦
    print("\nâš¡ æ¸¬è©¦2: JIT Loading æ•ˆèƒ½æ¸¬è©¦")
    for i, model in enumerate(test_models, 1):
        print(f"\n{i}. è¼‰å…¥æ¨¡å‹: {model}")
        load_time, success, used_model = test_model_loading(chat_url, model)
        
        if success:
            print(f"   âœ… æˆåŠŸè¼‰å…¥: {used_model}")
            print(f"   â±ï¸  è¼‰å…¥æ™‚é–“: {load_time:.1f}ç§’")
            
            # åˆ†æè¼‰å…¥é¡å‹
            if load_time > 10.0:
                print("   ğŸ¯ JIT Loading å·²å•Ÿç”¨ (è¼‰å…¥æ™‚é–“ >10ç§’)")
            elif load_time > 5.0:
                print("   âš¡ JIT Loading å¯èƒ½å•Ÿç”¨ (è¼‰å…¥æ™‚é–“ 5-10ç§’)")
            elif load_time > 2.0:
                print("   âš ï¸  éƒ¨åˆ†è¼‰å…¥æˆ–å¿«å– (è¼‰å…¥æ™‚é–“ 2-5ç§’)")
            else:
                print("   âŒ æ¨¡å‹å·²åœ¨è¨˜æ†¶é«”ä¸­ (è¼‰å…¥æ™‚é–“ <2ç§’)")
        else:
            print(f"   âŒ è¼‰å…¥å¤±æ•—")
    
    # æ¸¬è©¦3: Auto-Evict é©—è­‰
    print("\nğŸ”„ æ¸¬è©¦3: Auto-Evict åŠŸèƒ½é©—è­‰")
    auto_evict_result = test_auto_evict_behavior(chat_url, test_models)
    
    # æ¸¬è©¦4: TTL æ¸¬è©¦ (çŸ­ TTL)
    print("\nâ° æ¸¬è©¦4: TTL (Time-To-Live) åŠŸèƒ½æ¸¬è©¦")
    ttl_result = test_ttl_behavior(chat_url, test_models[0])
    
    # ç¶œåˆçµæœåˆ†æ
    print("\n" + "=" * 60)
    print("ğŸ“‹ æ¸¬è©¦çµæœç¸½çµ")
    print("=" * 60)
    
    analyze_results(memory_status, auto_evict_result, ttl_result)
    
    # æä¾›é…ç½®å»ºè­°
    print("\nğŸ’¡ é…ç½®å»ºè­°")
    print("-" * 30)
    provide_configuration_recommendations(auto_evict_result)


def check_memory_status(chat_url: str) -> Dict[str, Any]:
    """æª¢æŸ¥ç•¶å‰è¨˜æ†¶é«”ä¸­çš„æ¨¡å‹ç‹€æ…‹"""
    try:
        # ç™¼é€ç„¡æŒ‡å®šæ¨¡å‹çš„è«‹æ±‚
        response = requests.post(chat_url, json={
            'messages': [{'role': 'user', 'content': 'test'}],
            'max_tokens': 1
        }, timeout=10)
        
        if response.status_code == 404:
            error_data = response.json()
            error_message = error_data.get('error', {}).get('message', '')
            
            if 'Multiple models are loaded' in error_message:
                # è§£æè¼‰å…¥çš„æ¨¡å‹åˆ—è¡¨
                models_in_memory = []
                lines = error_message.split('\n')
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('Multiple') and not line.startswith('Your models'):
                        models_in_memory.append(line)
                
                return {
                    'status': 'multiple_models_loaded',
                    'models_count': len(models_in_memory),
                    'models': models_in_memory,
                    'auto_evict_enabled': False
                }
            else:
                return {
                    'status': 'no_models_loaded',
                    'models_count': 0,
                    'auto_evict_enabled': True
                }
        
        elif response.status_code == 200:
            result = response.json()
            return {
                'status': 'single_model_loaded',
                'models_count': 1,
                'default_model': result.get('model'),
                'auto_evict_enabled': True
            }
        
        else:
            return {'status': 'error', 'error': f'HTTP {response.status_code}'}
            
    except Exception as e:
        return {'status': 'error', 'error': str(e)}


def test_model_loading(chat_url: str, model_name: str, ttl: int = 1800) -> tuple:
    """æ¸¬è©¦å–®ä¸€æ¨¡å‹è¼‰å…¥æ™‚é–“"""
    try:
        start_time = time.time()
        
        response = requests.post(chat_url, json={
            'model': model_name,
            'messages': [{'role': 'user', 'content': 'test'}],
            'max_tokens': 1,
            'temperature': 0,
            'ttl': ttl
        }, timeout=120)  # çµ¦ JIT Loading è¶³å¤ æ™‚é–“ (2åˆ†é˜)
        
        load_time = time.time() - start_time
        
        if response.status_code == 200:
            result = response.json()
            used_model = result.get('model', model_name)
            return load_time, True, used_model
        else:
            return load_time, False, None
            
    except Exception as e:
        return 0, False, str(e)


def test_auto_evict_behavior(chat_url: str, models: list) -> Dict[str, Any]:
    """æ¸¬è©¦ Auto-Evict è¡Œç‚º"""
    if len(models) < 2:
        return {'error': 'Need at least 2 models for testing'}
    
    print(f"   ğŸ“ ä¾åºè¼‰å…¥å…©å€‹æ¨¡å‹æ¸¬è©¦ Auto-Evict...")
    
    # è¼‰å…¥ç¬¬ä¸€å€‹æ¨¡å‹
    load_time1, success1, model1 = test_model_loading(chat_url, models[0])
    print(f"   1ï¸âƒ£ {models[0]}: {load_time1:.1f}ç§’")
    
    if not success1:
        return {'error': f'Failed to load first model: {models[0]}'}
    
    # ç«‹å³è¼‰å…¥ç¬¬äºŒå€‹æ¨¡å‹
    load_time2, success2, model2 = test_model_loading(chat_url, models[1])
    print(f"   2ï¸âƒ£ {models[1]}: {load_time2:.1f}ç§’")
    
    if not success2:
        return {'error': f'Failed to load second model: {models[1]}'}
    
    # æª¢æŸ¥è¨˜æ†¶é«”ç‹€æ…‹
    memory_status = check_memory_status(chat_url)
    
    # åˆ†æçµæœ
    auto_evict_working = False
    if memory_status.get('status') == 'single_model_loaded':
        auto_evict_working = True
    elif memory_status.get('models_count', 0) <= 1:
        auto_evict_working = True
    
    return {
        'first_load_time': load_time1,
        'second_load_time': load_time2,
        'auto_evict_working': auto_evict_working,
        'final_memory_status': memory_status,
        'jit_loading_detected': load_time2 > 8.0
    }


def test_ttl_behavior(chat_url: str, model_name: str) -> Dict[str, Any]:
    """æ¸¬è©¦ TTL è¡Œç‚ºï¼ˆä½¿ç”¨çŸ­ TTL é€²è¡Œå¿«é€Ÿæ¸¬è©¦ï¼‰"""
    short_ttl = 10  # 10ç§’ TTL ç”¨æ–¼å¿«é€Ÿæ¸¬è©¦
    
    print(f"   â±ï¸ è¼‰å…¥æ¨¡å‹ä¸¦è¨­å®š {short_ttl}ç§’ TTL...")
    
    # è¼‰å…¥æ¨¡å‹ä¸¦è¨­å®šçŸ­ TTL
    load_time, success, used_model = test_model_loading(chat_url, model_name, short_ttl)
    
    if not success:
        return {'error': 'Failed to load model with TTL'}
    
    print(f"   âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œç­‰å¾… {short_ttl + 5}ç§’å¾Œæ¸¬è©¦æ˜¯å¦è¢«å¸è¼‰...")
    time.sleep(short_ttl + 5)  # ç­‰å¾… TTL éæœŸ
    
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦è¢«å¸è¼‰
    memory_status = check_memory_status(chat_url)
    
    # å†æ¬¡è¼‰å…¥åŒä¸€æ¨¡å‹ï¼ˆæ‡‰è©²è§¸ç™¼ JITï¼‰
    reload_time, reload_success, _ = test_model_loading(chat_url, model_name)
    
    return {
        'initial_load_time': load_time,
        'ttl_seconds': short_ttl,
        'memory_after_ttl': memory_status,
        'reload_time': reload_time,
        'ttl_working': reload_time > 8.0,  # é‡æ–°è¼‰å…¥æ™‚é–“é•·è¡¨ç¤º TTL ç”Ÿæ•ˆ
        'reload_success': reload_success
    }


def analyze_results(memory_status: Dict, auto_evict_result: Dict, ttl_result: Dict) -> None:
    """åˆ†ææ¸¬è©¦çµæœ"""
    
    print(f"ğŸ” è¨˜æ†¶é«”ç‹€æ…‹: {memory_status.get('status', 'unknown')}")
    if memory_status.get('models_count'):
        print(f"   æ¨¡å‹æ•¸é‡: {memory_status['models_count']}")
    
    if 'auto_evict_working' in auto_evict_result:
        auto_evict_status = "âœ… å·²å•Ÿç”¨" if auto_evict_result['auto_evict_working'] else "âŒ æœªå•Ÿç”¨"
        print(f"ğŸ”„ Auto-Evict: {auto_evict_status}")
        
        if auto_evict_result.get('jit_loading_detected'):
            print("âš¡ JIT Loading: âœ… å·²å•Ÿç”¨")
        else:
            print("âš¡ JIT Loading: âŒ æœªæª¢æ¸¬åˆ°")
    
    if 'ttl_working' in ttl_result:
        ttl_status = "âœ… æ­£å¸¸é‹ä½œ" if ttl_result['ttl_working'] else "âŒ æœªç”Ÿæ•ˆ"
        print(f"â° TTL åŠŸèƒ½: {ttl_status}")


def provide_configuration_recommendations(auto_evict_result: Dict) -> None:
    """æä¾›é…ç½®å»ºè­°"""
    
    if not auto_evict_result.get('auto_evict_working', False):
        print("âŒ Auto-Evict æœªå•Ÿç”¨ï¼Œè«‹åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿ:")
        print("   1. æ‰“é–‹ LM Studio æ‡‰ç”¨ç¨‹å¼")
        print("   2. å‰å¾€ Developer æ¨™ç±¤é ")
        print("   3. åœ¨ Server Settings ä¸­å•Ÿç”¨ Auto-Evict")
        print("   4. é‡å•Ÿ LM Studio Server")
        print("   5. é‡æ–°åŸ·è¡Œæ­¤æ¸¬è©¦")
    else:
        print("âœ… Auto-Evict é…ç½®æ­£ç¢º")
    
    if not auto_evict_result.get('jit_loading_detected', False):
        print("âŒ JIT Loading æœªæª¢æ¸¬åˆ°ï¼Œå»ºè­°:")
        print("   1. ç¢ºèª LM Studio ç‰ˆæœ¬ >= 0.3.9")
        print("   2. æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ‰‹å‹•é è¼‰å…¥")
        print("   3. é‡å•Ÿ LM Studio æ¸…é™¤è¨˜æ†¶é«”")
    else:
        print("âœ… JIT Loading é‹ä½œæ­£å¸¸")


if __name__ == "__main__":
    test_auto_evict_configuration()