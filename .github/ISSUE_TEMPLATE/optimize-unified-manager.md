---
name: Code Optimization - unified_manager.py
about: Optimize unified_manager.py following Linus methodology (332 lines â†’ <300 lines)
title: 'refactor: Extract service discovery and caching from unified_manager.py'
labels: 'refactoring, technical-debt, medium-priority, prevention'
assignees: ''
---

## ğŸ“‹ å•é¡Œæè¿°

**ç•¶å‰ç‹€æ…‹ï¼š**
- æ–‡ä»¶ï¼š`mcp/llm/unified_manager.py`
- è¡Œæ•¸ï¼š332 / 300ï¼ˆè¶…æ¨™ 10%ï¼‰ğŸŸ¡
- é¡åˆ¥ï¼š`UnifiedModelManager`ï¼ˆç®¡ç†å™¨é¡ï¼‰
- è·è²¬ï¼šæœå‹™ç™¼ç¾ + æ¨¡å‹ç®¡ç† + ç·©å­˜ç®¡ç† + å¥åº·æª¢æŸ¥

**è­¦å‘Šç‹€æ…‹ï¼š**
æ ¹æ“š [CLAUDE.md](../../CLAUDE.md) Linus æ–¹æ³•è«–ï¼š
- âš ï¸ æ–‡ä»¶è¡Œæ•¸ï¼š332 / 300ï¼ˆè‡¨ç•Œé»ï¼‰
- âš ï¸ å†åŠ ä»»ä½•æ–°åŠŸèƒ½éƒ½æœƒè¶…æ¨™
- âš ï¸ è·è²¬é–‹å§‹æ··äº‚ï¼ˆç®¡ç†å™¨åšäº†å¤ªå¤šäº‹ï¼‰

**æ ¸å¿ƒå•é¡Œï¼š**
```
ã€å“å‘³è©•åˆ†ã€‘
ğŸŸ¡ æ¹Šåˆ - 332 è¡Œå‹‰å¼·å¯ä»¥æ¥å—ï¼Œä½†å†åŠ åŠŸèƒ½å°±ç‚¸äº†

ã€è­¦å‘Šã€‘
- é€™æ˜¯å€‹ç®¡ç†å™¨é¡ï¼Œè·è²¬ç›¸å°å–®ä¸€
- ä½†å·²ç¶“åœ¨è‡¨ç•Œé»ï¼Œä»»ä½•æ–°åŠŸèƒ½éƒ½æœƒè®“å®ƒçˆ†ç‚¸
- ç¾åœ¨é‚„ä¾†å¾—åŠé é˜²æ€§é‡æ§‹
```

## ğŸ¯ é‡æ§‹ç›®æ¨™

**å¯æ¸¬é‡æŒ‡æ¨™ï¼š**
- âœ… ä¸»æ–‡ä»¶ < 200 è¡Œ
- âœ… æ¯å€‹é¡åˆ¥è·è²¬å–®ä¸€
- âœ… æœå‹™ç™¼ç¾é‚è¼¯ç¨ç«‹
- âœ… æ¨¡å‹ç·©å­˜é‚è¼¯ç¨ç«‹

**é æœŸçµæ§‹ï¼š**
```
mcp/llm/
â”œâ”€â”€ unified_manager.py (180 è¡Œ) - ç°¡åŒ–çš„ç®¡ç†å™¨
â”œâ”€â”€ service_discovery.py (100 è¡Œ) - æœå‹™ç™¼ç¾é‚è¼¯
â”œâ”€â”€ model_cache.py (120 è¡Œ) - æ¨¡å‹ç·©å­˜ç®¡ç†
â””â”€â”€ health_checker.py (80 è¡Œ) - å¥åº·æª¢æŸ¥é‚è¼¯
```

**è¨­è¨ˆåŸå‰‡ï¼š**
- UnifiedModelManager åªåš**å”èª¿**ï¼Œä¸åšå…·é«”å·¥ä½œ
- ä¾è³´æ³¨å…¥ï¼šå¯ä»¥æ›¿æ› ServiceDiscovery/ModelCache å¯¦ä½œ
- å–®ä¸€è·è²¬ï¼šæ¯å€‹é¡åˆ¥åªåšä¸€ä»¶äº‹

## ğŸ”§ å¯¦ä½œæ­¥é©Ÿ

### Phase 1: æå–æœå‹™ç™¼ç¾é‚è¼¯

**æ­¥é©Ÿ 1.1: å‰µå»º ServiceDiscovery é¡åˆ¥**
å‰µå»º `mcp/llm/service_discovery.py`ï¼š
```python
"""LLM æœå‹™ç™¼ç¾æ¨¡çµ„"""
import asyncio
import logging
from typing import Dict, Optional
from enum import Enum

logger = logging.getLogger(__name__)


class LLMServiceType(Enum):
    """LLM æœå‹™é¡å‹"""
    OLLAMA = "ollama"
    LMSTUDIO = "lmstudio"
    AUTO = "auto"


class ServiceDiscovery:
    """LLM æœå‹™ç™¼ç¾å™¨"""

    def __init__(self, ollama_client, lmstudio_client):
        self.ollama_client = ollama_client
        self.lmstudio_client = lmstudio_client
        self._available_services: Dict[LLMServiceType, bool] = {}

    async def discover_services(self) -> Dict[LLMServiceType, bool]:
        """ç™¼ç¾æ‰€æœ‰å¯ç”¨çš„ LLM æœå‹™

        Returns:
            Dict[LLMServiceType, bool]: æœå‹™å¯ç”¨æ€§æ˜ å°„
        """
        await asyncio.gather(
            self._check_ollama(),
            self._check_lmstudio(),
            return_exceptions=True
        )
        return self._available_services

    async def _check_ollama(self) -> None:
        """æª¢æŸ¥ Ollama æœå‹™å¯ç”¨æ€§"""
        try:
            healthy = await self.ollama_client.health_check()
            self._available_services[LLMServiceType.OLLAMA] = healthy
            if healthy:
                logger.info("Ollama service is available")
        except Exception as e:
            logger.warning(f"Ollama service not available: {e}")
            self._available_services[LLMServiceType.OLLAMA] = False

    async def _check_lmstudio(self) -> None:
        """æª¢æŸ¥ LM Studio æœå‹™å¯ç”¨æ€§"""
        try:
            healthy = await self.lmstudio_client.health_check()
            self._available_services[LLMServiceType.LMSTUDIO] = healthy
            if healthy:
                logger.info("LM Studio service is available")
        except Exception as e:
            logger.warning(f"LM Studio service not available: {e}")
            self._available_services[LLMServiceType.LMSTUDIO] = False

    def is_service_available(self, service_type: LLMServiceType) -> bool:
        """æª¢æŸ¥ç‰¹å®šæœå‹™æ˜¯å¦å¯ç”¨"""
        return self._available_services.get(service_type, False)

    def get_available_services(self) -> list[LLMServiceType]:
        """å–å¾—æ‰€æœ‰å¯ç”¨æœå‹™åˆ—è¡¨"""
        return [
            service for service, available
            in self._available_services.items()
            if available
        ]
```

### Phase 2: æå–æ¨¡å‹ç·©å­˜é‚è¼¯

**æ­¥é©Ÿ 2.1: å‰µå»º ModelCache é¡åˆ¥**
å‰µå»º `mcp/llm/model_cache.py`ï¼š
```python
"""æ¨¡å‹ç·©å­˜ç®¡ç†æ¨¡çµ„"""
import asyncio
import logging
from typing import Dict, List, Optional
from datetime import datetime, timedelta
from .types import ModelInfo
from .service_discovery import LLMServiceType

logger = logging.getLogger(__name__)


class ModelCache:
    """æ¨¡å‹ä¿¡æ¯ç·©å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_ttl: int = 300):
        """
        Args:
            cache_ttl: ç·©å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰ï¼Œé è¨­ 5 åˆ†é˜
        """
        self.cache_ttl = cache_ttl
        self._service_models: Dict[LLMServiceType, List[ModelInfo]] = {}
        self._model_cache: Dict[str, datetime] = {}
        self._lock = asyncio.Lock()

    async def get_models(
        self,
        service_type: LLMServiceType,
        force_refresh: bool = False
    ) -> List[ModelInfo]:
        """å–å¾—æœå‹™çš„æ¨¡å‹åˆ—è¡¨

        Args:
            service_type: LLM æœå‹™é¡å‹
            force_refresh: æ˜¯å¦å¼·åˆ¶åˆ·æ–°ç·©å­˜

        Returns:
            List[ModelInfo]: æ¨¡å‹åˆ—è¡¨
        """
        async with self._lock:
            if force_refresh or self._is_cache_expired(service_type):
                return []  # éœ€è¦åˆ·æ–°
            return self._service_models.get(service_type, [])

    async def set_models(
        self,
        service_type: LLMServiceType,
        models: List[ModelInfo]
    ) -> None:
        """è¨­ç½®æœå‹™çš„æ¨¡å‹åˆ—è¡¨

        Args:
            service_type: LLM æœå‹™é¡å‹
            models: æ¨¡å‹åˆ—è¡¨
        """
        async with self._lock:
            self._service_models[service_type] = models
            self._model_cache[service_type.value] = datetime.now()
            logger.debug(f"Cached {len(models)} models for {service_type.value}")

    def _is_cache_expired(self, service_type: LLMServiceType) -> bool:
        """æª¢æŸ¥ç·©å­˜æ˜¯å¦éæœŸ"""
        cache_time = self._model_cache.get(service_type.value)
        if not cache_time:
            return True

        age = datetime.now() - cache_time
        return age > timedelta(seconds=self.cache_ttl)

    async def clear_cache(self, service_type: Optional[LLMServiceType] = None) -> None:
        """æ¸…é™¤ç·©å­˜

        Args:
            service_type: æŒ‡å®šæœå‹™é¡å‹ï¼ŒNone å‰‡æ¸…é™¤å…¨éƒ¨
        """
        async with self._lock:
            if service_type:
                self._service_models.pop(service_type, None)
                self._model_cache.pop(service_type.value, None)
            else:
                self._service_models.clear()
                self._model_cache.clear()
```

### Phase 3: ç°¡åŒ– UnifiedModelManager

**æ­¥é©Ÿ 3.1: é‡æ§‹ä¸»ç®¡ç†å™¨**
ä¿®æ”¹ `mcp/llm/unified_manager.py`ï¼š
```python
"""çµ±ä¸€æ¨¡å‹ç®¡ç†å™¨"""
import asyncio
import logging
from typing import Optional

from ..config import get_config
from ..exceptions import MCPLLMError
from .client import OllamaClient
from .lmstudio_client import LMStudioClient
from .service_discovery import ServiceDiscovery, LLMServiceType
from .model_cache import ModelCache

logger = logging.getLogger(__name__)


class UnifiedModelManager:
    """çµ±ä¸€æ¨¡å‹ç®¡ç†å™¨ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""

    def __init__(
        self,
        preferred_service: LLMServiceType = LLMServiceType.AUTO,
        cache_ttl: int = 300
    ):
        self.config = get_config()
        self.preferred_service = preferred_service

        # å®¢æˆ¶ç«¯
        self._ollama_client: Optional[OllamaClient] = None
        self._lmstudio_client: Optional[LMStudioClient] = None

        # ä¾è³´æ³¨å…¥çš„çµ„ä»¶
        self.service_discovery: Optional[ServiceDiscovery] = None
        self.model_cache: Optional[ModelCache] = None

        self._initialized = False
        self._lock = asyncio.Lock()

    async def initialize(self) -> None:
        """åˆå§‹åŒ–çµ±ä¸€ç®¡ç†å™¨"""
        async with self._lock:
            if self._initialized:
                return

            try:
                # åˆå§‹åŒ–å®¢æˆ¶ç«¯
                self._ollama_client = OllamaClient()
                self._lmstudio_client = LMStudioClient()

                # åˆå§‹åŒ–çµ„ä»¶
                self.service_discovery = ServiceDiscovery(
                    self._ollama_client,
                    self._lmstudio_client
                )
                self.model_cache = ModelCache(cache_ttl=300)

                # ç™¼ç¾æœå‹™
                await self.service_discovery.discover_services()

                # è¼‰å…¥æ¨¡å‹
                await self._load_all_models()

                self._initialized = True
                logger.info("Unified model manager initialized successfully")

            except Exception as e:
                logger.error(f"Failed to initialize: {e}")
                raise MCPLLMError(f"Initialization failed: {e}") from e

    async def _load_all_models(self) -> None:
        """è¼‰å…¥æ‰€æœ‰å¯ç”¨æœå‹™çš„æ¨¡å‹"""
        available_services = self.service_discovery.get_available_services()

        for service_type in available_services:
            try:
                models = await self._fetch_models_for_service(service_type)
                await self.model_cache.set_models(service_type, models)
            except Exception as e:
                logger.warning(f"Failed to load models for {service_type}: {e}")

    async def _fetch_models_for_service(self, service_type: LLMServiceType):
        """ç²å–ç‰¹å®šæœå‹™çš„æ¨¡å‹åˆ—è¡¨"""
        if service_type == LLMServiceType.OLLAMA:
            return await self._ollama_client.list_models()
        elif service_type == LLMServiceType.LMSTUDIO:
            return await self._lmstudio_client.list_models()
        return []

    # ... å…¶ä»–æ–¹æ³•ä¿æŒç°¡æ½”
```

## âœ… é©—æ”¶æ¨™æº–

### ä»£ç¢¼å“è³ª
- [ ] unified_manager.py < 200 è¡Œ
- [ ] æ–°å¢æ–‡ä»¶å„ < 150 è¡Œ
- [ ] æ‰€æœ‰é¡åˆ¥è·è²¬å–®ä¸€
- [ ] mypy æª¢æŸ¥é€šéï¼ˆ0 éŒ¯èª¤ï¼‰
- [ ] ruff æª¢æŸ¥é€šéï¼ˆ0 è­¦å‘Šï¼‰

### åŠŸèƒ½æ¸¬è©¦
- [ ] æ‰€æœ‰ç¾æœ‰æ¸¬è©¦é€šé
- [ ] æœå‹™ç™¼ç¾æ­£å¸¸é‹ä½œ
- [ ] æ¨¡å‹åˆ—è¡¨ç²å–æ­£ç¢º
- [ ] ç·©å­˜æ©Ÿåˆ¶æ­£å¸¸
- [ ] å¥åº·æª¢æŸ¥åŠŸèƒ½æ­£å¸¸

### æ¶æ§‹æª¢æŸ¥
- [ ] UnifiedModelManager åªåšå”èª¿
- [ ] ServiceDiscovery å¯ç¨ç«‹æ¸¬è©¦
- [ ] ModelCache å¯ç¨ç«‹æ¸¬è©¦
- [ ] ä¾è³´æ³¨å…¥è¨­è¨ˆæ­£ç¢º

### å‘å¾Œç›¸å®¹æ€§
- [ ] ç¾æœ‰ API ç°½åä¸è®Š
- [ ] åŠŸèƒ½é›¶ç ´å£
- [ ] å°å…¥è·¯å¾‘ä¿æŒä¸€è‡´

## ğŸ”„ å›æ»¾è¨ˆåŠƒ

**Git æ¨™ç±¤ï¼š** `before-manager-optimize`

**å›æ»¾å‘½ä»¤ï¼š**
```bash
git tag before-manager-optimize  # å„ªåŒ–å‰æ‰“æ¨™ç±¤
git reset --hard before-manager-optimize  # å¦‚éœ€å›æ»¾
```

**é©—è­‰è…³æœ¬ï¼š**
```bash
# æª¢æŸ¥æ–‡ä»¶å¤§å°
wc -l mcp/llm/unified_manager.py
find mcp/llm -name "*.py" -exec wc -l {} +

# é‹è¡Œæ¸¬è©¦
docker compose exec django pytest mcp/tests/ -k "unified" -v

# æª¢æŸ¥ä»£ç¢¼å“è³ª
docker compose exec django ruff check mcp/llm/
docker compose exec django mypy mcp/llm/
```

## ğŸ“Š é æœŸæ”¶ç›Š

**å¯æ¸¬é‡æ”¹é€²ï¼š**
- ä¸»æ–‡ä»¶ï¼š332 è¡Œ â†’ ~180 è¡Œ âœ…
- æ–‡ä»¶æ•¸é‡ï¼š1 å€‹ â†’ 4 å€‹ï¼ˆè·è²¬æ¸…æ™°ï¼‰âœ…
- æœ€å¤§é¡åˆ¥ï¼šUnifiedModelManager â†’ ç°¡åŒ–çš„å”èª¿å™¨ âœ…
- æŠ€è¡“å‚µç­‰ç´šï¼šğŸŸ¡ MEDIUM â†’ ğŸŸ¢ LOW

**é•·æœŸåƒ¹å€¼ï¼š**
- âœ… é é˜²æ€§é‡æ§‹ï¼ˆè¶é‚„ç°¡å–®æ™‚è™•ç†ï¼‰
- âœ… æ›´å®¹æ˜“æ¸¬è©¦ï¼ˆçµ„ä»¶å¯ç¨ç«‹æ¸¬è©¦ï¼‰
- âœ… æ›´å®¹æ˜“æ“´å±•ï¼ˆæ–°å¢æœå‹™é¡å‹æ›´ç°¡å–®ï¼‰
- âœ… æ›´å®¹æ˜“ç¶­è­·ï¼ˆè·è²¬æ¸…æ™°ï¼‰

## ğŸ’¡ æœªä¾†æ“´å±•ç¤ºä¾‹

æ–°å¢æ”¯æ´ `OpenAI` æœå‹™ï¼ˆç¤ºä¾‹ï¼‰ï¼š
```python
# 1. åœ¨ ServiceDiscovery ä¸­æ·»åŠ 
async def _check_openai(self) -> None:
    """æª¢æŸ¥ OpenAI æœå‹™å¯ç”¨æ€§"""
    # å¯¦ä½œæª¢æŸ¥é‚è¼¯

# 2. åœ¨ discover_services ä¸­èª¿ç”¨
await asyncio.gather(
    self._check_ollama(),
    self._check_lmstudio(),
    self._check_openai(),  # æ–°å¢
    return_exceptions=True
)

# å®Œæˆï¼ç„¡éœ€ä¿®æ”¹ UnifiedModelManager
```

## ğŸ“š åƒè€ƒæ–‡æª”

- [CLAUDE.md - Linus æ–¹æ³•è«–](../../CLAUDE.md)
- [MCP é…ç½®æ–‡æª”](../../mcp/config.py)
- [LLM å®¢æˆ¶ç«¯æ–‡æª”](../../mcp/llm/client.py)

## ğŸ·ï¸ æ¨™ç±¤

`refactoring` `technical-debt` `medium-priority` `linus-methodology` `prevention` `week-3`

## â±ï¸ é ä¼°å·¥æ™‚

**2-3 å¤©**

**åˆ†è§£ï¼š**
- Day 1: å‰µå»º ServiceDiscovery å’Œ ModelCache
- Day 2: é‡æ§‹ UnifiedModelManager
- Day 3: æ¸¬è©¦ + æ–‡æª”æ›´æ–°
