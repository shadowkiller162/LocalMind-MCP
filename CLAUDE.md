# CLAUDE.md - AI Agent Development Framework

> **Purpose**: Universal development rules and guidelines for Claude Code CLI  
> **Scope**: Django & FastAPI projects  
> **Version**: v2.1  
> **Last Updated**: 2025-07-25

---

## ðŸš¨ AI PREFLIGHT CHECKLIST SYSTEM

> **Critical**: Execute this checklist BEFORE ANY code/file modification to prevent violations and ensure compliance with development standards.

### **âš¡ MANDATORY PREFLIGHT CHECKS**

#### 1. **ðŸ“‹ File Strategy Validation**
```
BEFORE MODIFYING/CREATING ANY FILE:
â–¡ Can I modify existing file instead of creating new one?
â–¡ Which files import/reference this module? (Check: grep -r "import.*[filename]" .)
â–¡ Will this modification break existing functionality?
â–¡ Is this the minimal change approach?

âŒ VIOLATION DETECTED â†’ Stop and ask human before proceeding
âœ… ALL CHECKS PASS â†’ Continue with modification
```

#### 2. **ðŸ³ Docker Environment Compliance**
```
BEFORE EXECUTING ANY COMMAND:
â–¡ Am I using `docker compose exec [service]` prefix?
â–¡ Am I avoiding direct host commands (python, pytest, etc.)?
â–¡ Is the correct service container specified?

âŒ VIOLATION DETECTED â†’ Stop and ask human before proceeding
âœ… ALL CHECKS PASS â†’ Continue with command execution
```

#### 3. **ðŸ“‹ Documentation Standards**
```
BEFORE CREATING/UPDATING DOCUMENTATION:
â–¡ Did I search for existing documentation files first?
â–¡ Am I updating existing files rather than creating new ones?
â–¡ Are my Git commit messages in English only?

âŒ VIOLATION DETECTED â†’ Stop and ask human before proceeding
âœ… ALL CHECKS PASS â†’ Continue with documentation update
```

### **ðŸ“Š PROJECT-SPECIFIC CONSTRAINTS**

#### **Current Project Context: LocalMind-MCP**
```yaml
framework: Django
key_files:
  - ai_agent_automation.py    # Main automation orchestrator
  - progress_updater.py       # Documentation updater
  - test_result_parser.py     # Test result processor
  - framework_detection.py   # Framework detection logic

critical_dependencies:
  - "ai_agent_automation.py:21 â†’ from test_result_parser import TestResultParser"
  - "progress_updater.py:15 â†’ from progress_updater import TestResults"
  
test_command: "docker compose exec django pytest"
docker_service: "django"
```

### **ðŸ¤– AI DECISION PROTOCOL**

When ANY modification is needed, AI MUST explicitly state:

```markdown
ðŸ” PREFLIGHT CHECK EXECUTED:
- File Strategy: [âœ… PASS/âŒ FAIL - specific reason]
- Docker Compliance: [âœ… PASS/âŒ FAIL - specific reason]  
- Documentation Standards: [âœ… PASS/âŒ FAIL - specific reason]

ðŸ“Š IMPACT ANALYSIS:
- Files affected: [list all files that will be modified]
- Dependencies broken: [list any import/reference breaks]
- Risk level: [HIGH/MEDIUM/LOW]

ðŸ’¡ SOLUTION OPTIONS:
- Option A: [minimal change approach with justification]
- Option B: [alternative approach with trade-offs]
- Recommended: [chosen option with detailed reasoning]

ðŸŽ¯ HUMAN CONSULTATION REQUIRED: [YES/NO - if YES, explain why]
```

### **ðŸš€ EXECUTION TRIGGER MECHANISMS**

#### **1. Session Initialization Trigger**
At the start of EVERY Claude Code session, AI MUST load and acknowledge:

```markdown
ðŸ¤– AI AGENT INITIALIZED
ðŸ“‹ Loaded: CLAUDE.md Preflight Checklist System v2.1
ðŸŽ¯ Mode: Human-Assisted Development (HAD)
âœ… Ready to assist with rule-compliant development

Current Project Context Loaded:
- Framework: Django
- Docker Service: django  
- Key Dependencies: [list from PROJECT-SPECIFIC CONSTRAINTS]
```

#### **2. Pre-Modification Trigger**
BEFORE any file modification, AI MUST:

```markdown
âš ï¸  PREFLIGHT CHECK REQUIRED
I'm about to modify [filename]. Let me run the mandatory checks first...

[Execute checklist and display results]

ðŸ¤” HUMAN DECISION POINT:
Based on the above analysis, would you like me to:
A) Proceed with recommended approach
B) Use alternative approach  
C) Let you handle this manually
D) Discuss other options

Please select: [A/B/C/D]
```

#### **3. Violation Detection Trigger**
When violations are detected:

```markdown
ðŸš¨ RULE VIOLATION DETECTED

Violation Type: [specific rule broken]
Potential Impact: [consequences if ignored]
Compliant Alternative: [suggested solution]

ðŸ¤” HUMAN CHOICE REQUIRED:
1. Apply compliant solution automatically
2. Override violation with justification
3. Pause for manual handling
4. Update rule (if this is a new valid pattern)

Please respond with choice number: [1/2/3/4]
```

#### **4. Learning Feedback Trigger**
After human corrections or overrides:

```markdown
ðŸ“š LEARNING OPPORTUNITY DETECTED

Human Action: [what you did differently]
AI Suggestion: [what AI recommended]
Outcome: [result of human choice]

ðŸŽ¯ RULE UPDATE PROPOSAL:
Should this pattern be added to CLAUDE.md for future reference?
- New rule: [proposed addition]  
- Reason: [why this improves development]

Add to checklist? [Yes/No/Modify]
```

### **âš¡ IMMEDIATE ACTIVATION PROTOCOL**

To activate this system immediately, add this section to your Claude Code CLI startup prompt:

```markdown
## SYSTEM INITIALIZATION COMMAND
Load and execute CLAUDE.md Preflight Checklist System v2.1
- Enable Human-Assisted Development (HAD) mode
- Display session initialization trigger
- Prepare to execute preflight checks before any modification
- Ready violation detection and human consultation triggers
```

### **ðŸ”„ TRIGGER TESTING PROTOCOL**

To verify the system works correctly:

```markdown
TEST SEQUENCE:
1. Start new Claude Code session â†’ Should display initialization trigger
2. Request file modification â†’ Should execute preflight check
3. Introduce intentional violation â†’ Should trigger violation detection
4. Provide feedback/correction â†’ Should trigger learning feedback

PASS CRITERIA:
- All 4 triggers execute correctly
- Human decision points are clearly presented  
- No actions taken without human confirmation on violations
```

### **ðŸš¨ VIOLATION RESPONSE PROTOCOL**

When violations are detected:

#### **For HIGH RISK Changes:**
- âŒ **STOP IMMEDIATELY**
- ðŸ¤” **Ask Human**: "I detected [violation type]. This could [potential impact]. Should I proceed with [alternative approach] or would you prefer a different solution?"

#### **For MEDIUM RISK Changes:**
- âš ï¸ **Warn and Suggest**: "I notice this violates [rule]. I recommend [compliant alternative]. Shall I proceed with the compliant approach?"

#### **For LOW RISK Changes:**
- âœ… **Proceed with Note**: "Applied [compliant solution] to avoid [potential violation]."

### **ðŸ“ˆ EFFECTIVENESS METRICS**

Track these metrics to measure system improvement:

```yaml
token_efficiency:
  baseline_tokens: "[record initial session token usage]"
  optimized_tokens: "[record post-checklist token usage]"
  target_reduction: "30%"

file_management:
  duplicate_files_created: 0
  unnecessary_modifications: 0
  backward_compatibility_breaks: 0

human_intervention:
  violations_caught_pre_execution: "[count]"
  human_corrections_required: "[count]" 
  error_types_prevented: "[list categories]"

quality_indicators:
  import_dependency_errors: 0
  architecture_violations: 0
  rule_compliance_rate: ">95%"
```

---

## ðŸ¤– MANDATORY AI AGENT EXECUTION RULES

### 1. **ðŸ³ Docker Environment Enforcement**
```bash
# âœ… REQUIRED - All commands must execute in Docker
docker compose exec django python manage.py migrate
docker compose exec django pytest
docker compose exec app uvicorn main:app --reload

# âŒ FORBIDDEN - Never execute on host
python manage.py migrate
pytest
uvicorn main:app --reload
```

### 2. **ðŸŒ English-Only Git Standards**
```bash
# âœ… REQUIRED - Professional English commits
git commit -m "Implement user authentication system"
git commit -m "Add comprehensive API testing suite"

# âŒ FORBIDDEN - Non-English commits
git commit -m "å¯¦ä½œç”¨æˆ¶èªè­‰ç³»çµ±"
```

### 3. **ðŸ“‹ Update-Before-Create Documentation**
```bash
# âœ… REQUIRED - Check existing files first
find . -name "*.md" | grep -i [keyword]
# Then update existing documentation

# âŒ FORBIDDEN - Create without checking
touch NEW_FILE.md
```

---

## ðŸ“Š AUTOMATED PROGRESS TRACKING SYSTEM

### **AI Agent Auto-Update Triggers**
- âœ… **Function Implementation Complete**: Update `docs/ai_agent/development_log.md`
- âœ… **Tests Pass**: Update `docs/ai_agent/test_results.md`
- âœ… **Milestone Achieved**: Update `docs/ai_agent/milestone_tracking.md`
- âœ… **Integration Success**: Update `docs/ai_agent/progress_report.md`

### **Update Template Format**
```markdown
## [TIMESTAMP] Feature Implementation: [FEATURE_NAME]

### âœ… Completed Tasks
- [x] Core business logic
- [x] API endpoints (Django views / FastAPI routes)
- [x] Data models
- [x] Unit tests (>90% coverage)
- [x] Integration tests
- [x] Frontend integration tests (if applicable)

### ðŸ“Š Test Results
- Unit Tests: [PASS_COUNT]/[TOTAL_COUNT] passed
- Integration Tests: [PASS_COUNT]/[TOTAL_COUNT] passed
- Coverage: [PERCENTAGE]%

### ðŸ”„ Next Steps
- [ ] [Next milestone task]
```

---

## ðŸ—ï¸ UNIVERSAL PROJECT ARCHITECTURE

### **Framework-Agnostic Structure**
```
project_root/
â”œâ”€â”€ README.md                    # Human-readable project overview
â”œâ”€â”€ CLAUDE.md                    # AI Agent development guide
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ai_agent/                # AI Agent documentation
â”‚   â”‚   â”œâ”€â”€ development_log.md   # Auto-updated development history
â”‚   â”‚   â”œâ”€â”€ progress_report.md   # Auto-updated progress tracking
â”‚   â”‚   â”œâ”€â”€ milestone_tracking.md # Feature-based milestone tracking
â”‚   â”‚   â””â”€â”€ test_results.md      # Auto-updated test results
â”‚   â””â”€â”€ human/                   # Human-readable documentation
â”œâ”€â”€ tests/                       # Test suites
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â””â”€â”€ [framework_specific_structure]
```

### **Django-Specific Structure**
```
django_project/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ users/          # User management
â”‚   â”œâ”€â”€ core/           # Business logic
â”‚   â””â”€â”€ api/            # API endpoints
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings/
â”‚   â””â”€â”€ urls.py
â”œâ”€â”€ requirements/
â”‚   â”œâ”€â”€ base.txt
â”‚   â”œâ”€â”€ local.txt
â”‚   â””â”€â”€ production.txt
â””â”€â”€ docker-compose.yml
```

### **FastAPI-Specific Structure**
```
fastapi_project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/            # API routes
â”‚   â”œâ”€â”€ core/           # Business logic
â”‚   â”œâ”€â”€ models/         # Data models
â”‚   â””â”€â”€ services/       # Service layer
â”œâ”€â”€ tests/
â”œâ”€â”€ pyproject.toml      # Poetry/pip-tools configuration
â””â”€â”€ docker-compose.yml
```

---

## ðŸ§ª FUNCTION-BASED TDD MILESTONE FRAMEWORK

### **Milestone Template**
```markdown
## Milestone: [FUNCTION_NAME]

### ðŸ“‹ Function Scope
**Business Requirements**:
- [Primary business objective]
- [User story or use case]
- [Success criteria]

**Technical Requirements**:
- [API endpoints to implement]
- [Data models needed]
- [External integrations]

### ðŸ§ª Testing Requirements (MANDATORY)

#### Unit Tests (>90% Coverage)
- [ ] Core business logic functions
- [ ] Model validations and methods
- [ ] Utility functions and helpers

#### Integration Tests
- [ ] Database operations (Django ORM / SQLAlchemy)
- [ ] External API calls (mocked and real)
- [ ] Authentication and authorization

#### Framework-Specific Tests
**Django Projects**:
- [ ] View/ViewSet functionality
- [ ] URL routing
- [ ] Template rendering (if applicable)
- [ ] Admin interface (if applicable)

**FastAPI Projects**:
- [ ] Route handlers
- [ ] Dependency injection
- [ ] Request/Response validation
- [ ] OpenAPI documentation generation

#### Frontend Integration Tests (If Applicable)
- [ ] API response format validation
- [ ] Error handling and status codes
- [ ] Authentication flow
- [ ] Real browser/client testing

### âœ… Completion Criteria
- [ ] All tests pass (unit + integration + e2e)
- [ ] Code quality checks pass (ruff, mypy, etc.)
- [ ] API documentation updated
- [ ] Progress documentation auto-updated
- [ ] Git commit with English message
- [ ] Milestone tagged in Git

### ðŸ”„ Rollback Configuration
- **Git Tag**: `milestone-[function-name]`
- **Rollback Command**: `git reset --hard milestone-[function-name]`
- **Verification Script**: `./scripts/verify_[function-name].sh`
```

---

## ðŸ”§ FRAMEWORK-SPECIFIC CONFIGURATIONS

### **Django Configuration Standards**
```python
# settings/base.py
import environ
env = environ.Env()

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': env('POSTGRES_DB'),
        'USER': env('POSTGRES_USER'),
        'PASSWORD': env('POSTGRES_PASSWORD'),
        'HOST': env('POSTGRES_HOST', default='localhost'),
        'PORT': env('POSTGRES_PORT', default='5432'),
    }
}

# Testing configuration
if 'test' in sys.argv:
    DATABASES['default'] = {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': ':memory:'
    }
```

### **FastAPI Configuration Standards**
```python
# app/core/config.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    database_url: str
    secret_key: str
    debug: bool = False
    
    class Config:
        env_file = ".env"

settings = Settings()

# app/main.py
from fastapi import FastAPI
from app.api import router

app = FastAPI(
    title="Project API",
    description="API documentation",
    version="1.0.0"
)

app.include_router(router, prefix="/api/v1")
```

---

## ðŸ“‹ DEVELOPMENT WORKFLOW (MANDATORY)

### **Pre-Development Checklist**
- [ ] Confirm execution in correct Docker container
- [ ] Check existing file structure to avoid duplication
- [ ] Prepare English commit message format
- [ ] Verify network configuration for containers
- [ ] Set appropriate timeout settings for external services

### **Development Cycle**
```
1. Update milestone in docs/ai_agent/milestone_tracking.md
2. Write failing tests (TDD approach)
3. Implement minimal viable solution
4. Pass all tests (unit + integration + e2e)
5. Update progress documentation (AUTO)
6. Commit with English message
7. Tag milestone in Git
8. Prepare next milestone
```

### **Post-Development Verification**
- [ ] All tests pass in Docker environment
- [ ] Git commit uses standard English format
- [ ] Documentation updated (not created anew)
- [ ] Configuration supports different environments
- [ ] Code quality meets project standards

---

## ðŸ” QUALITY ASSURANCE STANDARDS

### **Code Quality Tools**
```toml
# pyproject.toml (Both Django & FastAPI)
[tool.ruff]
line-length = 120
target-version = "py311"
extend-select = ["I", "N", "UP", "RUF"]

[tool.mypy]
python_version = "3.11"
check_untyped_defs = true
ignore_missing_imports = true

[tool.pytest.ini_options]
python_files = ["test_*.py", "*_test.py"]
addopts = "--cov=. --cov-report=html --cov-report=term-missing"
```

### **Testing Standards**
```python
# Test naming convention (Both frameworks)
def test_should_create_user_when_valid_data_provided():
    pass

def test_should_return_404_when_user_not_found():
    pass

# Django test example
from django.test import TestCase
from rest_framework.test import APITestCase

class UserAPITestCase(APITestCase):
    def test_should_authenticate_user_with_valid_credentials(self):
        pass

# FastAPI test example
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_should_return_user_profile():
    response = client.get("/api/v1/users/me")
    assert response.status_code == 200
```

---

## ðŸš¨ CONFLICT RESOLUTION: Django vs FastAPI

### **Identified Conflicts & Solutions**

#### 1. **Project Structure Differences**
**Conflict**: Django uses `apps/` directory, FastAPI uses `app/` single directory
**Solution**: Framework detection in AI Agent
```python
# AI Agent detection logic
if os.path.exists('manage.py'):
    framework = 'django'
    structure = 'apps/'
elif os.path.exists('main.py') or 'fastapi' in requirements:
    framework = 'fastapi'  
    structure = 'app/'
```

#### 2. **Testing Framework Differences**
**Conflict**: Django uses `django.test.TestCase`, FastAPI uses pure `pytest`
**Solution**: Framework-specific test templates
```python
# Django testing
from django.test import TestCase
from rest_framework.test import APITestCase

# FastAPI testing  
from fastapi.testclient import TestClient
import pytest
```

#### 3. **Dependency Management**
**Conflict**: Django typically uses `requirements.txt`, FastAPI often uses `pyproject.toml`
**Solution**: Support both formats
```bash
# Detection logic
if [ -f "pyproject.toml" ]; then
    poetry install
elif [ -f "requirements/base.txt" ]; then  
    pip install -r requirements/base.txt
fi
```

---

## ðŸŽ¯ PROJECT-SPECIFIC CONFIGURATION SECTION

> **Note**: This section contains LocalMind-MCP specific configurations and will be replaced with project-specific content in future projects.

### **LocalMind-MCP Specific Rules**
- MCP protocol implementation in `mcp/` directory
- Local LLM integration (Ollama, LM Studio)
- DeepSeek R1 model with thinking reasoning support
- Docker network configuration for `host.docker.internal`

### **Current Technical Stack**
- **Framework**: Django 4.2.23
- **Database**: PostgreSQL with Redis
- **Authentication**: JWT with djangorestframework-simplejwt
- **AI Services**: OpenAI, Anthropic, Google Generative AI
- **Local LLM**: Ollama, LM Studio integration
- **Testing**: pytest-django with factory_boy

---

## ðŸ“ˆ SUCCESS METRICS

A project following this framework should achieve:
- âœ… **100% Docker-based development** - No host machine execution
- âœ… **>90% test coverage** - Comprehensive testing suite  
- âœ… **English-only Git history** - Professional commit standards
- âœ… **Automated progress tracking** - Self-updating documentation
- âœ… **Function-based milestones** - Clear rollback points
- âœ… **Framework flexibility** - Django & FastAPI support

---

**Remember**: This framework is battle-tested from real project development. Every rule prevents specific production issues. Follow strictly for professional, maintainable applications.