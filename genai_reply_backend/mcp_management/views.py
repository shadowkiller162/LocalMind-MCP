"""
MCP Management Views

æä¾› MCP ç³»çµ±çš„ç®¡ç†ç•Œé¢ï¼ŒåŒ…å«ç‹€æ…‹ç›£æ§ã€é…ç½®ç®¡ç†ç­‰åŠŸèƒ½ã€‚
æ•´åˆ HTMX æä¾›ç¾ä»£åŒ–çš„ç”¨æˆ¶é«”é©—ã€‚
"""

import logging
import uuid
import requests
from datetime import datetime
from django.shortcuts import render
from django.http import JsonResponse, HttpResponse
from django.contrib.auth.decorators import login_required
from django.views.decorators.http import require_http_methods
from django_htmx.http import HttpResponseClientRedirect
from django.utils import timezone

logger = logging.getLogger(__name__)


def detect_lmstudio_models(config):
    """
    æª¢æ¸¬ LM Studio ä¸­å¯ç”¨çš„æ¨¡å‹
    
    Returns:
        dict: åŒ…å«æ¨¡å‹åˆ—è¡¨å’Œç‹€æ…‹ä¿¡æ¯
    """
    try:
        # LM Studio API endpoint for models
        url = f"http://{config.lmstudio_host}:{config.lmstudio_port}/v1/models"
        
        response = requests.get(
            url, 
            timeout=config.lmstudio_timeout or 10,
            headers={'Content-Type': 'application/json'}
        )
        
        if response.status_code == 200:
            models_data = response.json()
            
            # æå–æ¨¡å‹åç¨±åˆ—è¡¨
            if 'data' in models_data:
                models = [model.get('id', 'unknown') for model in models_data['data']]
                active_model = models[0] if models else config.default_model
                
                return {
                    'success': True,
                    'models': models,
                    'active_model': active_model,
                    'total_models': len(models),
                    'service_status': 'connected'
                }
            else:
                return {
                    'success': False,
                    'models': [config.default_model],
                    'active_model': config.default_model,
                    'error': 'Invalid response format from LM Studio',
                    'service_status': 'error'
                }
        else:
            return {
                'success': False,
                'models': [config.default_model],
                'active_model': config.default_model,
                'error': f'LM Studio API returned status {response.status_code}',
                'service_status': 'disconnected'
            }
            
    except requests.exceptions.ConnectionError:
        return {
            'success': False,
            'models': [config.default_model],
            'active_model': config.default_model,
            'error': 'Unable to connect to LM Studio. Is it running?',
            'service_status': 'disconnected'
        }
    except requests.exceptions.Timeout:
        return {
            'success': False,
            'models': [config.default_model],
            'active_model': config.default_model,
            'error': 'LM Studio connection timeout',
            'service_status': 'timeout'
        }
    except Exception as e:
        logger.error(f"LM Studio æ¨¡å‹æª¢æ¸¬éŒ¯èª¤: {e}")
        return {
            'success': False,
            'models': [config.default_model],
            'active_model': config.default_model,
            'error': f'Detection error: {str(e)}',
            'service_status': 'error'
        }


@login_required
def mcp_dashboard(request):
    """MCP ç®¡ç†å„€è¡¨æ¿ä¸»é é¢"""
    context = {
        'page_title': 'MCP Management Dashboard',
        'user': request.user,
    }
    return render(request, 'mcp_management/dashboard.html', context)


@login_required
@require_http_methods(["GET"])
def mcp_status(request):
    """
    MCP ç‹€æ…‹ç›£æ§ API
    
    è¿”å› MCP æœå‹™çš„å³æ™‚ç‹€æ…‹ä¿¡æ¯ï¼Œæ”¯æ´ HTMX è¼ªè©¢æ›´æ–°
    """
    try:
        # å¾ MCP é…ç½®æ¨¡çµ„å–å¾—ç‹€æ…‹ä¿¡æ¯
        from mcp.config import get_config
        from mcp.llm import UnifiedModelManager, LLMServiceType
        
        config = get_config()
        
        # æª¢æ¸¬ LM Studio ä¸­çš„å¯¦éš›æ¨¡å‹
        lmstudio_info = detect_lmstudio_models(config)
        
        # æ ¹æ“šæª¢æ¸¬çµæœæ§‹å»ºç‹€æ…‹ä¿¡æ¯
        is_connected = lmstudio_info['success']
        service_name = config.default_llm_service
        
        if is_connected:
            model_name = lmstudio_info['active_model']
            available_models = lmstudio_info['models']
            total_models = lmstudio_info['total_models']
        else:
            model_name = config.default_model
            available_models = [config.default_model]
            total_models = 1
        
        mcp_status = {
            'is_connected': is_connected,
            'service_name': service_name,
            'model_name': model_name,
            'available_models': available_models,
            'total_models': total_models,
            'last_ping': timezone.now().strftime('%Y-%m-%d %H:%M:%S'),
            'response_time': '150ms' if is_connected else 'N/A',
            'active_connections': 3 if is_connected else 0,
            'max_connections': config.max_connections,
            'lmstudio_status': lmstudio_info['service_status'],
            'error_message': lmstudio_info.get('error') if not is_connected else None,
        }
        
        if hasattr(request, 'htmx') and request.htmx:
            # HTMX è«‹æ±‚è¿”å›éƒ¨åˆ†æ¨¡æ¿
            return render(request, 'components/mcp_status.html', {
                'mcp_status': mcp_status
            })
        else:
            # æ™®é€šè«‹æ±‚è¿”å› JSON
            return JsonResponse(mcp_status, json_dumps_params={'ensure_ascii': False})
            
    except Exception as e:
        logger.error(f"ç²å– MCP ç‹€æ…‹å¤±æ•—: {e}")
        
        error_status = {
            'is_connected': False,
            'service_name': 'Unknown',
            'error_message': str(e),
        }
        
        if hasattr(request, 'htmx') and request.htmx:
            return render(request, 'components/mcp_status.html', {
                'mcp_status': error_status
            })
        else:
            return JsonResponse(error_status, status=500, json_dumps_params={'ensure_ascii': False})


@login_required
@require_http_methods(["POST"])
def mcp_reconnect(request):
    """
    MCP é‡æ–°é€£æ¥åŠŸèƒ½
    
    å˜—è©¦é‡æ–°å»ºç«‹ MCP é€£æ¥
    """
    try:
        # å¯¦ä½œ MCP é‡æ–°é€£æ¥é‚è¼¯
        success = True  # æ¨¡æ“¬æˆåŠŸ
        
        if success:
            if hasattr(request, 'htmx') and request.htmx:
                # è§¸ç™¼ç‹€æ…‹çµ„ä»¶åˆ·æ–°
                response = render(request, 'components/mcp_status.html', {
                    'mcp_status': {
                        'is_connected': True,
                        'service_name': 'ollama',
                        'reconnected': True,
                    }
                })
                response['HX-Trigger'] = 'mcpReconnected'
                return response
            else:
                return JsonResponse({'success': True, 'message': 'MCP é‡æ–°é€£æ¥æˆåŠŸ'}, json_dumps_params={'ensure_ascii': False})
        else:
            raise Exception("é‡æ–°é€£æ¥å¤±æ•—")
            
    except Exception as e:
        logger.error(f"MCP é‡æ–°é€£æ¥å¤±æ•—: {e}")
        
        if hasattr(request, 'htmx') and request.htmx:
            return render(request, 'components/mcp_status.html', {
                'mcp_status': {
                    'is_connected': False,
                    'error_message': f'é‡æ–°é€£æ¥å¤±æ•—: {str(e)}',
                }
            })
        else:
            return JsonResponse({'success': False, 'error': str(e)}, status=500, json_dumps_params={'ensure_ascii': False})


@login_required
@require_http_methods(["POST"])
def chat_send(request):
    """
    AI å°è©±ç™¼é€åŠŸèƒ½
    
    è™•ç†ç”¨æˆ¶ç™¼é€çš„å°è©±è¨Šæ¯ï¼Œèª¿ç”¨ MCP LLM æœå‹™é€²è¡Œå›è¦†
    """
    try:
        message_content = request.POST.get('message', '').strip()
        if not message_content:
            raise ValueError("è¨Šæ¯å…§å®¹ä¸èƒ½ç‚ºç©º")
        
        # å‰µå»ºç”¨æˆ¶è¨Šæ¯
        user_message = {
            'id': str(uuid.uuid4()),
            'content': message_content,
            'timestamp': timezone.now(),
            'type': 'user'
        }
        
        # æ¸²æŸ“ç”¨æˆ¶è¨Šæ¯
        user_message_html = render(request, 'partials/chat_message.html', {
            'message': user_message,
            'message_type': 'user'
        }).content.decode('utf-8')
        
        # å˜—è©¦èª¿ç”¨ MCP LLM æœå‹™
        try:
            ai_response = process_ai_message(message_content, request.user)
            
            # å‰µå»º AI å›è¦†è¨Šæ¯
            ai_message = {
                'id': str(uuid.uuid4()),
                'content': ai_response.get('content', 'æŠ±æ­‰ï¼Œæˆ‘æš«æ™‚ç„¡æ³•å›è¦†ã€‚'),
                'timestamp': timezone.now(),
                'thinking': ai_response.get('thinking'),  # DeepSeek R1 æ€è€ƒéç¨‹
                'mcp_data': ai_response.get('mcp_data'),
                'can_regenerate': True,
                'is_streaming': False
            }
            
            # æ¸²æŸ“ AI å›è¦†
            ai_message_html = render(request, 'partials/chat_message.html', {
                'message': ai_message,
                'message_type': 'assistant'
            }).content.decode('utf-8')
            
            # åˆä½µç”¨æˆ¶è¨Šæ¯å’Œ AI å›è¦†
            combined_html = clean_html_whitespace(user_message_html + ai_message_html)
            
            # HTMX æœŸæœ›ç›´æ¥çš„ HTML å›æ‡‰ï¼Œä¸æ˜¯ JSON
            return HttpResponse(combined_html, content_type='text/html')
            
        except Exception as ai_error:
            logger.error(f"AI è™•ç†éŒ¯èª¤: {ai_error}")
            
            # å‰µå»ºéŒ¯èª¤è¨Šæ¯
            error_message = {
                'id': str(uuid.uuid4()),
                'content': f'AI æœå‹™æš«æ™‚ä¸å¯ç”¨: {str(ai_error)}',
                'timestamp': timezone.now(),
                'error_details': str(ai_error) if request.user.is_staff else None
            }
            
            error_message_html = render(request, 'partials/chat_message.html', {
                'message': error_message,
                'message_type': 'error'
            }).content.decode('utf-8')
            
            combined_html = clean_html_whitespace(user_message_html + error_message_html)
            
            # HTMX éŒ¯èª¤æƒ…æ³ä¹Ÿå›å‚³ HTML
            return HttpResponse(combined_html, content_type='text/html')
        
    except Exception as e:
        logger.error(f"å°è©±è™•ç†éŒ¯èª¤: {e}")
        
        error_message = {
            'content': f'è™•ç†è¨Šæ¯æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}',
            'timestamp': timezone.now(),
        }
        
        error_html = clean_html_whitespace(render(request, 'partials/chat_message.html', {
            'message': error_message,
            'message_type': 'error'
        }).content.decode('utf-8'))
        
        # HTMX ç•°å¸¸æƒ…æ³ä¹Ÿå›å‚³ HTML
        return HttpResponse(error_html, content_type='text/html')


def clean_html_whitespace(html_content):
    """
    æ¸…ç† HTML å…§å®¹ä¸­çš„å¤šé¤˜æ›è¡Œå’Œç©ºç™½å­—ç¬¦
    
    å„ªåŒ–å‰ç«¯é¡¯ç¤ºé«”é©—ï¼Œç§»é™¤ä¸å¿…è¦çš„ç©ºç™½
    """
    import re
    
    if not html_content:
        return html_content
    
    # ç§»é™¤å¤šé¤˜çš„æ›è¡Œç¬¦å’Œç©ºæ ¼
    # ä¿ç•™æœ‰æ„ç¾©çš„æ›è¡Œï¼ˆåœ¨æ¨™ç±¤é–“ï¼‰ï¼Œä½†ç§»é™¤éå¤šçš„ç©ºç™½
    cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', html_content)  # åˆä½µå¤šå€‹ç©ºè¡Œ
    cleaned = re.sub(r'>\s*\n\s*<', '><', cleaned)  # ç§»é™¤æ¨™ç±¤é–“ç„¡æ„ç¾©æ›è¡Œ
    cleaned = re.sub(r'\n\s+', '\n', cleaned)  # ç§»é™¤è¡Œé¦–å¤šé¤˜ç©ºæ ¼
    cleaned = re.sub(r'\s+\n', '\n', cleaned)  # ç§»é™¤è¡Œå°¾å¤šé¤˜ç©ºæ ¼
    
    return cleaned.strip()


def parse_ai_response(raw_content):
    """
    è§£æ AI å›æ‡‰å…§å®¹ï¼Œåˆ†é›¢æ€è€ƒéç¨‹å’Œå¯¦éš›å›è¦†
    
    æ”¯æ´å¤šç¨®æ€è€ƒæ¨™ç±¤æ ¼å¼ï¼š
    - <think>...</think>
    - <thinking>...</thinking>
    - <!-- thinking -->...</think>
    """
    import re
    
    if not raw_content:
        return {'content': 'æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ã€‚', 'thinking': None}
    
    # å®šç¾©æ€è€ƒæ¨™ç±¤çš„æ­£å‰‡æ¨¡å¼
    thinking_patterns = [
        r'<think>(.*?)</think>',           # <think>...</think>
        r'<thinking>(.*?)</thinking>',     # <thinking>...</thinking>
        r'<!-- thinking -->(.*?)(?:</thinking>|$)',  # <!-- thinking -->...
    ]
    
    thinking_content = None
    cleaned_content = raw_content
    
    # å˜—è©¦åŒ¹é…å’Œæå–æ€è€ƒéç¨‹
    for pattern in thinking_patterns:
        match = re.search(pattern, raw_content, re.DOTALL | re.IGNORECASE)
        if match:
            thinking_content = match.group(1).strip()
            # å¾åŸå§‹å…§å®¹ä¸­ç§»é™¤æ€è€ƒæ¨™ç±¤å’Œå…§å®¹
            cleaned_content = re.sub(pattern, '', raw_content, flags=re.DOTALL | re.IGNORECASE).strip()
            break
    
    # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ€è€ƒæ¨™ç±¤ï¼Œæª¢æŸ¥æ˜¯å¦æ•´å€‹å…§å®¹éƒ½æ˜¯æ€è€ƒéç¨‹
    if not thinking_content and raw_content.strip().startswith('<think>'):
        # å¯èƒ½æ˜¯æœªé–‰åˆçš„æ€è€ƒæ¨™ç±¤
        thinking_match = re.search(r'<think>(.*)', raw_content, re.DOTALL | re.IGNORECASE)
        if thinking_match:
            thinking_content = thinking_match.group(1).strip()
            cleaned_content = 'æˆ‘æ­£åœ¨æ€è€ƒæ‚¨çš„å•é¡Œï¼Œè«‹ç¨å€™...'
    
    # æ¸…ç†å…§å®¹ä¸­çš„å¤šé¤˜ç©ºè¡Œå’Œæ ¼å¼
    if cleaned_content:
        cleaned_content = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned_content)  # åˆä½µå¤šå€‹ç©ºè¡Œ
        cleaned_content = cleaned_content.strip()
    
    # å¦‚æœæ¸…ç†å¾Œæ²’æœ‰å¯¦éš›å…§å®¹ï¼Œæä¾›é è¨­å›è¦†
    if not cleaned_content or len(cleaned_content.strip()) < 10:
        cleaned_content = 'æˆ‘æ­£åœ¨è™•ç†æ‚¨çš„è«‹æ±‚ï¼Œè«‹ç¨å€™...'
    
    return {
        'content': cleaned_content,
        'thinking': thinking_content
    }


def process_ai_message(message_content, user):
    """
    è™•ç† AI è¨Šæ¯çš„æ ¸å¿ƒé‚è¼¯
    
    é€™è£¡æ•´åˆ MCP LLM æœå‹™ä¾†è™•ç†ç”¨æˆ¶è¨Šæ¯
    """
    try:
        # æª¢æŸ¥æ˜¯å¦ç‚ºç³»çµ±ç›¸é—œæŸ¥è©¢
        if any(keyword in message_content.lower() for keyword in ['ç‹€æ…‹', 'status', 'mcp', 'æª¢æŸ¥']):
            return handle_system_query(message_content)
        
        # èª¿ç”¨ MCP LLM æœå‹™
        from mcp.config import get_config
        from mcp.llm import UnifiedModelManager, LLMServiceType
        from mcp.llm.types import ChatRequest, ChatMessage
        
        config = get_config()
        llm_manager = UnifiedModelManager(LLMServiceType.AUTO)
        
        # æº–å‚™å°è©±ä¸Šä¸‹æ–‡
        system_prompt = """ä½ æ˜¯ LocalMind-MCP å¹³å°çš„ AI åŠ©æ‰‹ã€‚ä½ å¯ä»¥ï¼š
1. å›ç­”é—œæ–¼ MCP (Model Context Protocol) çš„å•é¡Œ
2. å”åŠ©ç”¨æˆ¶ç®¡ç†å’Œé…ç½® MCP æœå‹™
3. æä¾›ç³»çµ±ä½¿ç”¨èªªæ˜å’ŒæŠ€è¡“æ”¯æ´
4. é€²è¡Œä¸€èˆ¬æ€§å°è©±

è«‹ç”¨å‹å–„ã€å°ˆæ¥­çš„èªæ°£å›ç­”ç”¨æˆ¶å•é¡Œã€‚"""
        
        # åˆå§‹åŒ–çµ±ä¸€ç®¡ç†å™¨ä¸¦ç™¼é€å°è©±è«‹æ±‚
        import asyncio
        
        async def get_ai_response():
            await llm_manager.initialize()
            
            # å‰µå»ºèŠå¤©è¨Šæ¯åˆ—è¡¨
            messages = [
                ChatMessage(role="system", content=system_prompt),
                ChatMessage(role="user", content=message_content)
            ]
            
            return await llm_manager.chat(
                model_name=config.default_model,
                messages=messages
            )
        
        # åŸ·è¡Œç•°æ­¥å°è©±è«‹æ±‚
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            response = loop.run_until_complete(get_ai_response())
        finally:
            loop.close()
        
        # è§£æå›æ‡‰å…§å®¹ï¼Œåˆ†é›¢æ€è€ƒéç¨‹å’Œå¯¦éš›å›è¦†
        parsed_response = parse_ai_response(response.content if hasattr(response, 'content') else 'æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚ã€‚')
        
        return {
            'content': parsed_response['content'],
            'thinking': parsed_response['thinking'] or getattr(response, 'thinking', None),  # DeepSeek R1 ç‰¹æœ‰
            'mcp_data': getattr(response, 'mcp_data', None),
            'model_used': getattr(response, 'model', config.default_model)
        }
        
    except Exception as e:
        logger.error(f"AI è™•ç†éŒ¯èª¤: {e}")
        raise e


def handle_system_query(message_content):
    """è™•ç†ç³»çµ±ç‹€æ…‹æŸ¥è©¢"""
    try:
        from mcp.config import get_config
        config = get_config()
        
        status_info = f"""ğŸ” **ç³»çµ±ç‹€æ…‹æª¢æŸ¥çµæœ**

**MCP æœå‹™é…ç½®:**
- é è¨­æœå‹™: {config.default_llm_service}
- é è¨­æ¨¡å‹: {config.default_model}
- æœ€å¤§é€£ç·šæ•¸: {config.max_connections}
- é€£ç·šé€¾æ™‚: {config.connection_timeout}ç§’

**å•Ÿç”¨çš„é€£æ¥å™¨:**
{', '.join(config.enabled_connectors)}

**å¿«å–ç‹€æ…‹:**
- å•Ÿç”¨å¿«å–: {'æ˜¯' if config.enable_cache else 'å¦'}
- å¿«å– TTL: {config.cache_ttl}ç§’

ç³»çµ±ç›®å‰é‹è¡Œæ­£å¸¸ï¼å¦‚éœ€æ›´è©³ç´°çš„è³‡è¨Šï¼Œè«‹æŸ¥çœ‹ MCP ç®¡ç†å„€è¡¨æ¿ã€‚"""

        return {
            'content': status_info,
            'mcp_data': {
                'connector_type': 'system',
                'operation': 'status_check'
            }
        }
        
    except Exception as e:
        return {
            'content': f'ç„¡æ³•ç²å–ç³»çµ±ç‹€æ…‹: {str(e)}',
            'mcp_data': {
                'connector_type': 'system',
                'operation': 'status_check_failed'
            }
        }


@login_required
@require_http_methods(["POST"])
def chat_regenerate(request):
    """é‡æ–°ç”Ÿæˆ AI å›è¦†"""
    try:
        message_id = request.POST.get('message_id')
        # é€™è£¡å¯ä»¥å¯¦ç¾é‡æ–°ç”Ÿæˆé‚è¼¯
        # æš«æ™‚è¿”å›ä¸€å€‹æ¨¡æ“¬çš„é‡æ–°ç”Ÿæˆå›è¦†
        
        regenerated_message = {
            'id': message_id,
            'content': 'é€™æ˜¯é‡æ–°ç”Ÿæˆçš„å›è¦†ã€‚(åŠŸèƒ½é–‹ç™¼ä¸­)',
            'timestamp': timezone.now(),
            'can_regenerate': True,
            'is_streaming': False
        }
        
        return render(request, 'partials/chat_message.html', {
            'message': regenerated_message,
            'message_type': 'assistant'
        })
        
    except Exception as e:
        logger.error(f"é‡æ–°ç”Ÿæˆå¤±æ•—: {e}")
        return JsonResponse({'error': str(e)}, status=500, json_dumps_params={'ensure_ascii': False})