#!/usr/bin/env python3
"""
LM Studio æ•´åˆæ‰‹å‹•æ¸¬è©¦è…³æœ¬

æ¸¬è©¦ LM Studio èˆ‡ MCP ç³»çµ±çš„æ•´åˆåŠŸèƒ½ã€‚
æ­¤è…³æœ¬éœ€è¦ LM Studio æ­£åœ¨é‹è¡Œä¸¦è¼‰å…¥äº†æ¨¡å‹æ‰èƒ½åŸ·è¡Œå®Œæ•´æ¸¬è©¦ã€‚

ä½¿ç”¨æ–¹æ³•:
docker compose exec django python -m mcp.tests.manual_test_lmstudio

å‰ç½®æ¢ä»¶:
1. LM Studio 0.3.20 å·²å®‰è£ä¸¦é‹è¡Œ
2. å·²è¼‰å…¥ DeepSeek-R1-Distill-Qwen-7B æˆ–å…¶ä»–æ¨¡å‹
3. LM Studio çš„ Local Server å·²å•Ÿå‹• (é€šå¸¸åœ¨ http://localhost:1234)
"""

import asyncio
import sys
from mcp.llm.lmstudio_client import LMStudioClient
from mcp.llm.unified_manager import UnifiedModelManager, LLMServiceType
from mcp.llm.types import ChatMessage


async def test_lmstudio_client():
    """æ¸¬è©¦ LM Studio å®¢æˆ¶ç«¯åŸºæœ¬åŠŸèƒ½"""
    print("=== LM Studio å®¢æˆ¶ç«¯æ¸¬è©¦ ===")
    
    client = LMStudioClient()
    
    # 1. æ¸¬è©¦å¥åº·æª¢æŸ¥
    print("1. æ¸¬è©¦ LM Studio æœå‹™å¥åº·æª¢æŸ¥...")
    try:
        health = await client.health_check()
        print(f"   å¥åº·æª¢æŸ¥çµæœ: {'âœ… æ­£å¸¸' if health else 'âŒ ç•°å¸¸'}")
        
        if not health:
            print("   âš ï¸  LM Studio æœå‹™æœªé‹è¡Œï¼Œè·³éå¾ŒçºŒæ¸¬è©¦")
            print("   ğŸ’¡ è«‹ç¢ºä¿:")
            print("      1. LM Studio æ‡‰ç”¨ç¨‹å¼å·²å•Ÿå‹•")
            print("      2. è‡³å°‘è¼‰å…¥ä¸€å€‹æ¨¡å‹")
            print("      3. 'Local Server' å·²å•Ÿå‹• (é è¨­ç«¯å£ 1234)")
            return False
    except Exception as e:
        print(f"   âŒ å¥åº·æª¢æŸ¥å¤±æ•—: {e}")
        print("   âš ï¸  è«‹ç¢ºä¿ LM Studio çš„ Local Server æ­£åœ¨é‹è¡Œ")
        return False
    
    # 2. æ¸¬è©¦æ¨¡å‹åˆ—è¡¨
    print("\n2. æ¸¬è©¦æ¨¡å‹åˆ—è¡¨...")
    try:
        async with client:
            models = await client.list_models()
            print(f"   æ‰¾åˆ° {len(models)} å€‹æ¨¡å‹:")
            
            deepseek_found = False
            for i, model in enumerate(models):
                print(f"     {i+1}. {model.name}")
                if model.details:
                    print(f"        æ“æœ‰è€…: {model.details.get('owned_by', 'unknown')}")
                    print(f"        ç‰©ä»¶é¡å‹: {model.details.get('object', 'unknown')}")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ DeepSeek æ¨¡å‹
                if "deepseek" in model.name.lower():
                    deepseek_found = True
                    print(f"        ğŸ¯ ç™¼ç¾ DeepSeek æ¨¡å‹ï¼")
            
            if deepseek_found:
                print("   âœ… æ‰¾åˆ° DeepSeek æ¨¡å‹")
            else:
                print("   âš ï¸  æœªæ‰¾åˆ° DeepSeek æ¨¡å‹ï¼Œä½†æœ‰å…¶ä»–å¯ç”¨æ¨¡å‹")
                
    except Exception as e:
        print(f"   âŒ ç²å–æ¨¡å‹åˆ—è¡¨å¤±æ•—: {e}")
        return False
    
    # 3. æ¸¬è©¦æ–‡æœ¬ç”Ÿæˆ
    print("\n3. æ¸¬è©¦æ–‡æœ¬ç”Ÿæˆ...")
    try:
        async with client:
            models = await client.list_models()
            if not models:
                print("   âš ï¸  æ²’æœ‰å¯ç”¨æ¨¡å‹é€²è¡Œæ¸¬è©¦")
                return False
            
            test_model = models[0].name
            print(f"   ä½¿ç”¨æ¨¡å‹: {test_model}")
            
            from mcp.llm.types import GenerateRequest
            request = GenerateRequest(
                model=test_model,
                prompt="Hello! Please introduce yourself in one sentence.",
            )
            
            print("   ç™¼é€è«‹æ±‚...")
            response = await client.generate(request)
            
            print(f"   âœ… ç”ŸæˆæˆåŠŸ:")
            print(f"     æ¨¡å‹: {response.model}")
            print(f"     å›æ‡‰: {response.content[:200]}...")
            print(f"     å®Œæˆç‹€æ…‹: {'å·²å®Œæˆ' if response.done else 'é€²è¡Œä¸­'}")
            
            if response.prompt_eval_count:
                print(f"     æç¤º Token æ•¸: {response.prompt_eval_count}")
            if response.eval_count:
                print(f"     ç”Ÿæˆ Token æ•¸: {response.eval_count}")
                
    except Exception as e:
        print(f"   âŒ æ–‡æœ¬ç”Ÿæˆå¤±æ•—: {e}")
        return False
    
    # 4. æ¸¬è©¦èŠå¤©åŠŸèƒ½
    print("\n4. æ¸¬è©¦èŠå¤©åŠŸèƒ½...")
    try:
        async with client:
            models = await client.list_models()
            test_model = models[0].name
            
            messages = [
                ChatMessage(role="system", content="You are a helpful AI assistant."),
                ChatMessage(role="user", content="What is the capital of Taiwan? Answer in one sentence.")
            ]
            
            from mcp.llm.types import ChatRequest
            request = ChatRequest(
                model=test_model,
                messages=messages
            )
            
            print(f"   ä½¿ç”¨æ¨¡å‹: {test_model}")
            print("   ç™¼é€èŠå¤©è«‹æ±‚...")
            
            response = await client.chat(request)
            
            print(f"   âœ… èŠå¤©æˆåŠŸ:")
            print(f"     æ¨¡å‹: {response.model}")
            print(f"     å›æ‡‰: {response.content}")
            
    except Exception as e:
        print(f"   âŒ èŠå¤©åŠŸèƒ½æ¸¬è©¦å¤±æ•—: {e}")
        return False
    
    return True


async def test_unified_manager():
    """æ¸¬è©¦çµ±ä¸€æ¨¡å‹ç®¡ç†å™¨"""
    print("\n=== çµ±ä¸€æ¨¡å‹ç®¡ç†å™¨æ¸¬è©¦ ===")
    
    # æ¸¬è©¦è‡ªå‹•æª¢æ¸¬æœå‹™
    manager = UnifiedModelManager(preferred_service=LLMServiceType.AUTO)
    
    # 1. æ¸¬è©¦åˆå§‹åŒ–
    print("1. æ¸¬è©¦ç®¡ç†å™¨åˆå§‹åŒ–...")
    try:
        await manager.initialize()
        print("   âœ… åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"   âŒ åˆå§‹åŒ–å¤±æ•—: {e}")
        return False
    
    # 2. æ¸¬è©¦æœå‹™å¯ç”¨æ€§æª¢æŸ¥
    print("\n2. æ¸¬è©¦æœå‹™å¯ç”¨æ€§...")
    try:
        services = await manager.get_available_services()
        print("   å¯ç”¨æœå‹™:")
        for service, available in services.items():
            status = "âœ… å¯ç”¨" if available else "âŒ ä¸å¯ç”¨"
            print(f"     {service}: {status}")
        
        available_count = sum(1 for available in services.values() if available)
        if available_count > 0:
            print(f"   âœ… æ‰¾åˆ° {available_count} å€‹å¯ç”¨æœå‹™")
        else:
            print("   âš ï¸  æ²’æœ‰å¯ç”¨çš„ LLM æœå‹™")
            return False
            
    except Exception as e:
        print(f"   âŒ æœå‹™æª¢æŸ¥å¤±æ•—: {e}")
        return False
    
    # 3. æ¸¬è©¦æ¨¡å‹åˆ—è¡¨
    print("\n3. æ¸¬è©¦çµ±ä¸€æ¨¡å‹åˆ—è¡¨...")
    try:
        models = await manager.list_models()
        print(f"   æ‰¾åˆ° {len(models)} å€‹æ¨¡å‹:")
        
        ollama_count = 0
        lmstudio_count = 0
        
        for model in models[:5]:  # åªé¡¯ç¤ºå‰5å€‹
            print(f"     - {model.name}")
            if "ollama:" in model.name:
                ollama_count += 1
            elif "lmstudio:" in model.name:
                lmstudio_count += 1
        
        print(f"   æœå‹™åˆ†å¸ƒ:")
        print(f"     Ollama: {ollama_count} å€‹æ¨¡å‹")
        print(f"     LM Studio: {lmstudio_count} å€‹æ¨¡å‹")
        
    except Exception as e:
        print(f"   âŒ æ¨¡å‹åˆ—è¡¨ç²å–å¤±æ•—: {e}")
        return False
    
    # 4. æ¸¬è©¦æ¨è–¦æ¨¡å‹
    print("\n4. æ¸¬è©¦æ¨è–¦æ¨¡å‹...")
    try:
        recommended = await manager.get_recommended_model()
        if recommended:
            print(f"   æ¨è–¦æ¨¡å‹: {recommended}")
            
            # æ¸¬è©¦æ¨è–¦æ¨¡å‹æ˜¯å¦åŒ…å« DeepSeek
            if "deepseek" in recommended.lower():
                print("   ğŸ¯ æ¨è–¦çš„æ˜¯ DeepSeek æ¨¡å‹ï¼")
            elif "lmstudio" in recommended:
                print("   âœ… æ¨è–¦çš„æ˜¯ LM Studio æ¨¡å‹")
            else:
                print("   âœ… æ‰¾åˆ°æ¨è–¦æ¨¡å‹")
        else:
            print("   âš ï¸  æ²’æœ‰å¯æ¨è–¦çš„æ¨¡å‹")
            
    except Exception as e:
        print(f"   âŒ æ¨è–¦æ¨¡å‹ç²å–å¤±æ•—: {e}")
        return False
    
    # 5. æ¸¬è©¦ä½¿ç”¨æ¨è–¦æ¨¡å‹é€²è¡Œå°è©±
    print("\n5. æ¸¬è©¦æ¨è–¦æ¨¡å‹å°è©±...")
    try:
        recommended = await manager.get_recommended_model()
        if recommended:
            messages = [
                ChatMessage(role="user", content="ä½ å¥½ï¼è«‹ç”¨ä¸€å¥è©±è‡ªæˆ‘ä»‹ç´¹ã€‚")
            ]
            
            print(f"   ä½¿ç”¨æ¨¡å‹: {recommended}")
            response = await manager.chat(recommended, messages)
            
            print(f"   âœ… å°è©±æˆåŠŸ:")
            print(f"     å›æ‡‰: {response.content[:150]}...")
        else:
            print("   âš ï¸  æ²’æœ‰å¯ç”¨æ¨¡å‹é€²è¡Œæ¸¬è©¦")
            
    except Exception as e:
        print(f"   âŒ å°è©±æ¸¬è©¦å¤±æ•—: {e}")
        return False
    
    # 6. æ¸¬è©¦å¥åº·æª¢æŸ¥
    print("\n6. æ¸¬è©¦å¥åº·æª¢æŸ¥...")
    try:
        health_status = await manager.health_check()
        print("   å¥åº·æª¢æŸ¥çµæœ:")
        for service, healthy in health_status.items():
            status = "âœ… å¥åº·" if healthy else "âŒ ç•°å¸¸"
            print(f"     {service}: {status}")
            
    except Exception as e:
        print(f"   âŒ å¥åº·æª¢æŸ¥å¤±æ•—: {e}")
        return False
    
    # 7. æ¸…ç†
    await manager.cleanup()
    print("   âœ… ç®¡ç†å™¨æ¸…ç†å®Œæˆ")
    
    return True


async def test_deepseek_specific():
    """å°ˆé–€æ¸¬è©¦ DeepSeek æ¨¡å‹åŠŸèƒ½"""
    print("\n=== DeepSeek æ¨¡å‹å°ˆé–€æ¸¬è©¦ ===")
    
    manager = UnifiedModelManager(preferred_service=LLMServiceType.AUTO)
    
    try:
        await manager.initialize()
        
        # æŸ¥æ‰¾ DeepSeek æ¨¡å‹
        models = await manager.list_models()
        deepseek_models = [
            model for model in models 
            if "deepseek" in model.name.lower()
        ]
        
        if not deepseek_models:
            print("   âš ï¸  æœªæ‰¾åˆ° DeepSeek æ¨¡å‹")
            print("   ğŸ’¡ è«‹ç¢ºä¿åœ¨ LM Studio ä¸­è¼‰å…¥äº† DeepSeek-R1-Distill-Qwen-7B æ¨¡å‹")
            return False
        
        print(f"   æ‰¾åˆ° {len(deepseek_models)} å€‹ DeepSeek æ¨¡å‹:")
        for model in deepseek_models:
            print(f"     - {model.name}")
        
        # ä½¿ç”¨ç¬¬ä¸€å€‹ DeepSeek æ¨¡å‹é€²è¡Œæ¸¬è©¦
        test_model = deepseek_models[0].name
        print(f"\n   ä½¿ç”¨æ¨¡å‹é€²è¡Œæ·±åº¦æ¸¬è©¦: {test_model}")
        
        # æ¸¬è©¦ä¸­æ–‡å°è©±
        print("\n   æ¸¬è©¦ä¸­æ–‡å°è©±èƒ½åŠ›...")
        messages = [
            ChatMessage(role="user", content="è«‹ç”¨ç¹é«”ä¸­æ–‡ä»‹ç´¹ä¸€ä¸‹å°ç£çš„ç¾é£Ÿæ–‡åŒ–ï¼Œå¤§ç´„50å­—ã€‚")
        ]
        
        response = await manager.chat(test_model, messages)
        print(f"   ä¸­æ–‡å›æ‡‰: {response.content}")
        
        # æ¸¬è©¦ç¨‹å¼ç¢¼ç”Ÿæˆ
        print("\n   æ¸¬è©¦ç¨‹å¼ç¢¼ç”Ÿæˆèƒ½åŠ›...")
        messages = [
            ChatMessage(role="user", content="è«‹å¯«ä¸€å€‹ Python å‡½æ•¸ä¾†è¨ˆç®—æ–æ³¢é‚£å¥‘æ•¸åˆ—çš„ç¬¬ n é …ï¼Œä¸¦æä¾›ç°¡å–®çš„æ¸¬è©¦ã€‚")
        ]
        
        response = await manager.chat(test_model, messages)
        print(f"   ç¨‹å¼ç¢¼å›æ‡‰: {response.content[:300]}...")
        
        # æ¸¬è©¦é‚è¼¯æ¨ç†
        print("\n   æ¸¬è©¦é‚è¼¯æ¨ç†èƒ½åŠ›...")
        messages = [
            ChatMessage(role="user", content="å¦‚æœä»Šå¤©æ˜¯æ˜ŸæœŸä¸‰ï¼Œé‚£éº¼ä¸‰å¤©å¾Œæ˜¯æ˜ŸæœŸå¹¾ï¼Ÿè«‹è§£é‡‹ä½ çš„æ¨ç†éç¨‹ã€‚")
        ]
        
        response = await manager.chat(test_model, messages)
        print(f"   æ¨ç†å›æ‡‰: {response.content}")
        
        print("   âœ… DeepSeek æ¨¡å‹åŠŸèƒ½æ¸¬è©¦å®Œæˆ")
        
        await manager.cleanup()
        return True
        
    except Exception as e:
        print(f"   âŒ DeepSeek æ¸¬è©¦å¤±æ•—: {e}")
        return False


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹ LM Studio æ•´åˆæ‰‹å‹•æ¸¬è©¦")
    print("=" * 60)
    
    print("ğŸ“‹ æ¸¬è©¦ç’°å¢ƒæª¢æŸ¥:")
    print("   - LM Studio ç‰ˆæœ¬: 0.3.20")
    print("   - é æœŸæ¨¡å‹: DeepSeek-R1-Distill-Qwen-7B-GGUF")
    print("   - é æœŸç«¯å£: http://localhost:1234")
    
    tests = [
        ("LM Studio å®¢æˆ¶ç«¯", test_lmstudio_client),
        ("çµ±ä¸€æ¨¡å‹ç®¡ç†å™¨", test_unified_manager),
        ("DeepSeek æ¨¡å‹å°ˆé–€æ¸¬è©¦", test_deepseek_specific),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ åŸ·è¡Œæ¸¬è©¦: {test_name}")
        try:
            if await test_func():
                passed += 1
                print(f"âœ… {test_name} æ¸¬è©¦é€šé")
            else:
                print(f"âŒ {test_name} æ¸¬è©¦å¤±æ•—")
        except Exception as e:
            print(f"âŒ {test_name} æ¸¬è©¦ç•°å¸¸: {e}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ æ¸¬è©¦å®Œæˆ: {passed}/{total} é€šé")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰ LM Studio æ•´åˆæ¸¬è©¦é€šéï¼")
        print("\nğŸ’¡ æ‚¨çš„ DeepSeek æ¨¡å‹å·²æº–å‚™å°±ç·’ï¼Œå¯ä»¥:")
        print("   1. æ•´åˆåˆ° MCP ä¼ºæœå™¨ä¸­")
        print("   2. åœ¨ Django å¾Œç«¯ä¸­ä½¿ç”¨")
        print("   3. æä¾›çµ¦å‰ç«¯æ‡‰ç”¨ç¨‹å¼å‘¼å«")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æ¸¬è©¦å¤±æ•—")
        print("\nğŸ”§ ä¿®å¾©å»ºè­°:")
        print("   1. ç¢ºä¿ LM Studio æ‡‰ç”¨ç¨‹å¼æ­£åœ¨é‹è¡Œ")
        print("   2. ç¢ºä¿è‡³å°‘è¼‰å…¥ä¸€å€‹æ¨¡å‹")
        print("   3. ç¢ºä¿ 'Local Server' å·²å•Ÿå‹•")
        print("   4. æª¢æŸ¥ç«¯å£ 1234 æ˜¯å¦è¢«å…¶ä»–ç¨‹å¼å ç”¨")
        return False


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)